{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e219c6-20f4-4ae7-92ef-52ed4fb25d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43456 images belonging to 38 classes.\n",
      "Found 10849 images belonging to 38 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1us/step  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,766</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m327,936\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)                  │           \u001b[38;5;34m9,766\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,595,686</span> (9.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,595,686\u001b[0m (9.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">337,702</span> (1.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m337,702\u001b[0m (1.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARADHYA\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1421s\u001b[0m 2s/step - accuracy: 0.5073 - loss: 1.8996 - val_accuracy: 0.8131 - val_loss: 0.7244\n",
      "Epoch 2/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1302s\u001b[0m 2s/step - accuracy: 0.7389 - loss: 0.8914 - val_accuracy: 0.8815 - val_loss: 0.4337\n",
      "Epoch 3/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1511s\u001b[0m 2s/step - accuracy: 0.7996 - loss: 0.6570 - val_accuracy: 0.9034 - val_loss: 0.3361\n",
      "Epoch 4/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1483s\u001b[0m 2s/step - accuracy: 0.8278 - loss: 0.5574 - val_accuracy: 0.9141 - val_loss: 0.2904\n",
      "Epoch 5/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1268s\u001b[0m 2s/step - accuracy: 0.8489 - loss: 0.4847 - val_accuracy: 0.9212 - val_loss: 0.2571\n",
      "Epoch 6/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 2s/step - accuracy: 0.8594 - loss: 0.4453 - val_accuracy: 0.9262 - val_loss: 0.2385\n",
      "Epoch 7/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1279s\u001b[0m 2s/step - accuracy: 0.8664 - loss: 0.4141 - val_accuracy: 0.9311 - val_loss: 0.2228\n",
      "Epoch 8/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1301s\u001b[0m 2s/step - accuracy: 0.8776 - loss: 0.3844 - val_accuracy: 0.9319 - val_loss: 0.2123\n",
      "Epoch 9/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1225s\u001b[0m 2s/step - accuracy: 0.8832 - loss: 0.3648 - val_accuracy: 0.9370 - val_loss: 0.2000\n",
      "Epoch 10/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 2s/step - accuracy: 0.8860 - loss: 0.3492 - val_accuracy: 0.9373 - val_loss: 0.1984\n",
      "Epoch 11/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 2s/step - accuracy: 0.8908 - loss: 0.3372 - val_accuracy: 0.9411 - val_loss: 0.1853\n",
      "Epoch 12/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1283s\u001b[0m 2s/step - accuracy: 0.8957 - loss: 0.3225 - val_accuracy: 0.9401 - val_loss: 0.1817\n",
      "Epoch 13/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1267s\u001b[0m 2s/step - accuracy: 0.8973 - loss: 0.3132 - val_accuracy: 0.9420 - val_loss: 0.1818\n",
      "Epoch 14/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 2s/step - accuracy: 0.9000 - loss: 0.3048 - val_accuracy: 0.9425 - val_loss: 0.1748\n",
      "Epoch 15/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 2s/step - accuracy: 0.9031 - loss: 0.2963 - val_accuracy: 0.9458 - val_loss: 0.1695\n",
      "Epoch 16/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1266s\u001b[0m 2s/step - accuracy: 0.9066 - loss: 0.2901 - val_accuracy: 0.9476 - val_loss: 0.1662\n",
      "Epoch 17/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 2s/step - accuracy: 0.9049 - loss: 0.2896 - val_accuracy: 0.9457 - val_loss: 0.1668\n",
      "Epoch 18/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 2s/step - accuracy: 0.9086 - loss: 0.2779 - val_accuracy: 0.9461 - val_loss: 0.1643\n",
      "Epoch 19/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1369s\u001b[0m 2s/step - accuracy: 0.9098 - loss: 0.2724 - val_accuracy: 0.9487 - val_loss: 0.1589\n",
      "Epoch 20/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1666s\u001b[0m 2s/step - accuracy: 0.9112 - loss: 0.2711 - val_accuracy: 0.9481 - val_loss: 0.1538\n",
      "Epoch 21/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1552s\u001b[0m 2s/step - accuracy: 0.9147 - loss: 0.2578 - val_accuracy: 0.9489 - val_loss: 0.1523\n",
      "Epoch 22/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1287s\u001b[0m 2s/step - accuracy: 0.9141 - loss: 0.2595 - val_accuracy: 0.9521 - val_loss: 0.1485\n",
      "Epoch 23/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 2s/step - accuracy: 0.9159 - loss: 0.2527 - val_accuracy: 0.9528 - val_loss: 0.1530\n",
      "Epoch 24/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1274s\u001b[0m 2s/step - accuracy: 0.9169 - loss: 0.2534 - val_accuracy: 0.9510 - val_loss: 0.1523\n",
      "Epoch 25/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1268s\u001b[0m 2s/step - accuracy: 0.9191 - loss: 0.2466 - val_accuracy: 0.9504 - val_loss: 0.1481\n",
      "Epoch 26/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2059s\u001b[0m 3s/step - accuracy: 0.9187 - loss: 0.2457 - val_accuracy: 0.9545 - val_loss: 0.1415\n",
      "Epoch 27/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 2s/step - accuracy: 0.9197 - loss: 0.2414 - val_accuracy: 0.9513 - val_loss: 0.1429\n",
      "Epoch 28/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1322s\u001b[0m 2s/step - accuracy: 0.9193 - loss: 0.2432 - val_accuracy: 0.9504 - val_loss: 0.1465\n",
      "Epoch 29/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1276s\u001b[0m 2s/step - accuracy: 0.9209 - loss: 0.2363 - val_accuracy: 0.9527 - val_loss: 0.1455\n",
      "Epoch 30/30\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1320s\u001b[0m 2s/step - accuracy: 0.9216 - loss: 0.2340 - val_accuracy: 0.9517 - val_loss: 0.1406\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "NUM_CLASSES = 38  # number of classes in your dataset\n",
    "\n",
    "# ✅ Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"Plant Village Dataset\", \n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    \"Plant Village Dataset\",  # same folder\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# ✅ Load Base Model Correctly\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top=False,     # exclude the final classification layer\n",
    "    weights='imagenet'     # use pretrained ImageNet weights\n",
    ")\n",
    "\n",
    "# ✅ Freeze base layers initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# ✅ Build Final Model\n",
    "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# ✅ Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ✅ Check Model Summary (now it should show params)\n",
    "model.summary()\n",
    "\n",
    "# ✅ Train\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5eafbc5-582c-4f74-8c64-3aeab7dc81ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully from: models\\model.keras\n",
      "✅ Class mapping saved successfully at: models\\class_mapping.json\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "🖼️ Image: Plant Village Dataset/Tomato___Late_blight/0a4b3cde-c83a-4c83-b037-010369738152___RS_Late.B 6985.JPG\n",
      "✅ Predicted: Tomato___Late_blight (92.11% confidence)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_save_path = models_dir / \"model.keras\"\n",
    "class_mapping_path = models_dir / \"class_mapping.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD MODEL\n",
    "# -----------------------------\n",
    "model = load_model(model_save_path)\n",
    "print(f\"✅ Model loaded successfully from: {model_save_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# CLASS MAPPING (38 classes)\n",
    "# -----------------------------\n",
    "class_mapping = {\n",
    "    \"0\": \"Apple___Apple_scab\",\n",
    "    \"1\": \"Apple___Black_rot\",\n",
    "    \"2\": \"Apple___Cedar_apple_rust\",\n",
    "    \"3\": \"Apple___healthy\",\n",
    "    \"4\": \"Blueberry___healthy\",\n",
    "    \"5\": \"Cherry_(including_sour)___Powdery_mildew\",\n",
    "    \"6\": \"Cherry_(including_sour)___healthy\",\n",
    "    \"7\": \"Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\",\n",
    "    \"8\": \"Corn_(maize)___Common_rust_\",\n",
    "    \"9\": \"Corn_(maize)___Northern_Leaf_Blight\",\n",
    "    \"10\": \"Corn_(maize)___healthy\",\n",
    "    \"11\": \"Grape___Black_rot\",\n",
    "    \"12\": \"Grape___Esca_(Black_Measles)\",\n",
    "    \"13\": \"Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\",\n",
    "    \"14\": \"Grape___healthy\",\n",
    "    \"15\": \"Orange___Haunglongbing_(Citrus_greening)\",\n",
    "    \"16\": \"Peach___Bacterial_spot\",\n",
    "    \"17\": \"Peach___healthy\",\n",
    "    \"18\": \"Pepper,_bell___Bacterial_spot\",\n",
    "    \"19\": \"Pepper,_bell___healthy\",\n",
    "    \"20\": \"Potato___Early_blight\",\n",
    "    \"21\": \"Potato___Late_blight\",\n",
    "    \"22\": \"Potato___healthy\",\n",
    "    \"23\": \"Raspberry___healthy\",\n",
    "    \"24\": \"Soybean___healthy\",\n",
    "    \"25\": \"Squash___Powdery_mildew\",\n",
    "    \"26\": \"Strawberry___Leaf_scorch\",\n",
    "    \"27\": \"Strawberry___healthy\",\n",
    "    \"28\": \"Tomato___Bacterial_spot\",\n",
    "    \"29\": \"Tomato___Early_blight\",\n",
    "    \"30\": \"Tomato___Late_blight\",\n",
    "    \"31\": \"Tomato___Leaf_Mold\",\n",
    "    \"32\": \"Tomato___Septoria_leaf_spot\",\n",
    "    \"33\": \"Tomato___Spider_mites Two-spotted_spider_mite\",\n",
    "    \"34\": \"Tomato___Target_Spot\",\n",
    "    \"35\": \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",\n",
    "    \"36\": \"Tomato___Tomato_mosaic_virus\",\n",
    "    \"37\": \"Tomato___healthy\"\n",
    "}\n",
    "\n",
    "with open(class_mapping_path, \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(class_mapping, f, indent=4)\n",
    "print(f\"✅ Class mapping saved successfully at: {class_mapping_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# TEST MODEL ON A SAMPLE IMAGE\n",
    "# -----------------------------\n",
    "def test_model(sample_image_path):\n",
    "    img = image.load_img(sample_image_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    idx = int(np.argmax(preds))\n",
    "    prob = float(preds[0, idx])\n",
    "    label = class_mapping[str(idx)]\n",
    "    print(f\"🖼️ Image: {sample_image_path}\")\n",
    "    print(f\"✅ Predicted: {label} ({prob*100:.2f}% confidence)\")\n",
    "\n",
    "# Example usage — change image path below:\n",
    "test_model(\"Plant Village Dataset/Tomato___Late_blight/0a4b3cde-c83a-4c83-b037-010369738152___RS_Late.B 6985.JPG\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b64c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded class mapping file: models\\class_mapping.json\n",
      "🔢 Total classes found: 38\n",
      "\n",
      "📋 Sample entries (first 10):\n",
      "0: Apple___Apple_scab\n",
      "1: Apple___Black_rot\n",
      "2: Apple___Cedar_apple_rust\n",
      "3: Apple___healthy\n",
      "4: Blueberry___healthy\n",
      "5: Cherry_(including_sour)___Powdery_mildew\n",
      "6: Cherry_(including_sour)___healthy\n",
      "7: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "8: Corn_(maize)___Common_rust_\n",
      "9: Corn_(maize)___Northern_Leaf_Blight\n",
      "\n",
      "✅ All class names are unique.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the class mapping file\n",
    "class_mapping_path = Path(\"models/class_mapping.json\")\n",
    "\n",
    "# Check if file exists\n",
    "if not class_mapping_path.exists():\n",
    "    print(\"❌ class_mapping.json not found! Please check the path.\")\n",
    "else:\n",
    "    # Load the JSON file\n",
    "    with open(class_mapping_path, \"r\", encoding=\"utf8\") as f:\n",
    "        class_mapping = json.load(f)\n",
    "\n",
    "    # Display number of classes and few examples\n",
    "    print(f\"✅ Successfully loaded class mapping file: {class_mapping_path}\")\n",
    "    print(f\"🔢 Total classes found: {len(class_mapping)}\")\n",
    "\n",
    "    # Show first 10 classes for verification\n",
    "    print(\"\\n📋 Sample entries (first 10):\")\n",
    "    for key in list(class_mapping.keys())[:10]:\n",
    "        print(f\"{key}: {class_mapping[key]}\")\n",
    "\n",
    "    # Optional: check for missing or duplicate labels\n",
    "    labels = list(class_mapping.values())\n",
    "    if len(labels) != len(set(labels)):\n",
    "        print(\"\\n⚠️ Warning: Duplicate class names detected!\")\n",
    "    else:\n",
    "        print(\"\\n✅ All class names are unique.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad244c4e-c005-4adb-9c20-f339deddfc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43456 images belonging to 38 classes.\n",
      "Found 10849 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "NUM_CLASSES = 38  # number of classes in your dataset\n",
    "\n",
    "# ✅ Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"Plant Village Dataset\", \n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    \"Plant Village Dataset\",  # same folder\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b92e640-964f-4672-bb76-9d219b107690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total classes: 38\n",
      "📋 Example class mapping:\n",
      "0 : Apple___Apple_scab\n",
      "1 : Apple___Black_rot\n",
      "2 : Apple___Cedar_apple_rust\n",
      "3 : Apple___healthy\n",
      "4 : Blueberry___healthy\n",
      "5 : Cherry_(including_sour)___Powdery_mildew\n",
      "6 : Cherry_(including_sour)___healthy\n",
      "7 : Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "8 : Corn_(maize)___Common_rust_\n",
      "9 : Corn_(maize)___Northern_Leaf_Blight\n",
      "10 : Corn_(maize)___healthy\n",
      "11 : Grape___Black_rot\n",
      "12 : Grape___Esca_(Black_Measles)\n",
      "13 : Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "14 : Grape___healthy\n",
      "15 : Orange___Haunglongbing_(Citrus_greening)\n",
      "16 : Peach___Bacterial_spot\n",
      "17 : Peach___healthy\n",
      "18 : Pepper,_bell___Bacterial_spot\n",
      "19 : Pepper,_bell___healthy\n",
      "20 : Potato___Early_blight\n",
      "21 : Potato___Late_blight\n",
      "22 : Potato___healthy\n",
      "23 : Raspberry___healthy\n",
      "24 : Soybean___healthy\n",
      "25 : Squash___Powdery_mildew\n",
      "26 : Strawberry___Leaf_scorch\n",
      "27 : Strawberry___healthy\n",
      "28 : Tomato___Bacterial_spot\n",
      "29 : Tomato___Early_blight\n",
      "30 : Tomato___Late_blight\n",
      "31 : Tomato___Leaf_Mold\n",
      "32 : Tomato___Septoria_leaf_spot\n",
      "33 : Tomato___Spider_mites Two-spotted_spider_mite\n",
      "34 : Tomato___Target_Spot\n",
      "35 : Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "36 : Tomato___Tomato_mosaic_virus\n",
      "37 : Tomato___healthy\n"
     ]
    }
   ],
   "source": [
    "# Get class indices (dictionary mapping class name → index)\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Invert it to get index → class name\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(\"✅ Total classes:\", len(index_to_class))\n",
    "print(\"📋 Example class mapping:\")\n",
    "for i in range(38):  # print first 10 classes\n",
    "    print(i, \":\", index_to_class[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae885e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
